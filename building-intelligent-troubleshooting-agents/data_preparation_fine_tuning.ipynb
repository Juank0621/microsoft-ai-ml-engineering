{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case Demonstration: Selecting and Preparing Data for Fine-tuning\n",
    "\n",
    "This notebook demonstrates the process of selecting and preparing data for fine-tuning a machine learning model, specifically for emotion intensity classification in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Modules\n",
    "\n",
    "First, let's install the necessary packages. The `!` prefix runs the command in the system shell rather than the Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install modules\n",
    "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
    "!pip install nltk transformers torch scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for data processing, model tokenization, and machine learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preview Dataset\n",
    "\n",
    "Load the tweet emotion intensity dataset. You can download this dataset from: https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
    "data = pd.read_csv('data/tweet_emotion_intensity.csv')\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())\n",
    "print(f\"\\nDataset shape: {data.shape}\")\n",
    "print(f\"\\nColumn names: {data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "Clean the text data by removing URLs, HTML tags, special characters, and converting to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabet characters\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Show examples of original vs cleaned text\n",
    "print(\"Original vs Cleaned Text Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {data['tweet'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {data['cleaned_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization with BERT\n",
    "\n",
    "Use the BERT tokenizer to convert text into tokens that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(data['cleaned_text'].tolist(), max_length=128, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Tokenized input shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {tokens['attention_mask'].shape}\")\n",
    "\n",
    "# Show an example of tokenization\n",
    "example_idx = 0\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"Original text: {data['cleaned_text'].iloc[example_idx]}\")\n",
    "print(f\"Token IDs: {tokens['input_ids'][example_idx][:20]}...\")  # Show first 20 tokens\n",
    "print(f\"Decoded tokens: {tokenizer.decode(tokens['input_ids'][example_idx][:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting\n",
    "\n",
    "Split the dataset into training (70%), validation (15%), and test (15%) sets using stratified sampling to maintain class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, stratify=data['sentiment_intensity'], random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['sentiment_intensity'], random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(train_data['sentiment_intensity'].value_counts().sort_index())\n",
    "print(\"\\nValidation set:\")\n",
    "print(val_data['sentiment_intensity'].value_counts().sort_index())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_data['sentiment_intensity'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation with Synonym Replacement\n",
    "\n",
    "Augment the training data by replacing some words with their synonyms to increase dataset diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wordnet if this is your first time using it\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def synonym_replacement(word):\n",
    "    \"\"\"Replace a word with its first synonym if available\"\"\"\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        return synonyms[0].lemmas()[0].name()\n",
    "    return word\n",
    "\n",
    "# Apply synonym replacement on random words in the dataset\n",
    "def augment_text(text, replacement_prob=0.2):\n",
    "    \"\"\"Augment text by randomly replacing words with synonyms\"\"\"\n",
    "    words = text.split()\n",
    "    augmented_words = [synonym_replacement(word) if random.random() < replacement_prob else word for word in words]\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply augmentation\n",
    "random.seed(42)  # For reproducibility\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)\n",
    "\n",
    "# Show examples of augmentation\n",
    "print(\"Text Augmentation Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {data['cleaned_text'].iloc[i]}\")\n",
    "    print(f\"Augmented: {data['augmented_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Label Preparation and Dataset Creation\n",
    "\n",
    "Convert sentiment intensity labels to numeric values and prepare the final dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs and labels into PyTorch tensors\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "\n",
    "# Define a mapping function for sentiment intensity\n",
    "def map_sentiment(value):\n",
    "    \"\"\"Map sentiment intensity categories to numeric values\"\"\"\n",
    "    if value == \"high\":\n",
    "        return 1\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Handle unexpected values, if any\n",
    "\n",
    "# Apply the function to each item in 'sentiment_intensity'\n",
    "data['sentiment_intensity_numeric'] = data['sentiment_intensity'].apply(map_sentiment)\n",
    "\n",
    "# Check for any null values after mapping\n",
    "print(f\"Null values after mapping: {data['sentiment_intensity_numeric'].isnull().sum()}\")\n",
    "print(f\"Value distribution after mapping:\")\n",
    "print(data['sentiment_intensity_numeric'].value_counts().sort_index())\n",
    "\n",
    "# Drop any rows where 'sentiment_intensity_numeric' is None\n",
    "data = data.dropna(subset=['sentiment_intensity_numeric']).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'sentiment_intensity_numeric' column to a tensor\n",
    "labels = torch.tensor(data['sentiment_intensity_numeric'].tolist(), dtype=torch.float32)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {data.shape}\")\n",
    "print(f\"Labels tensor shape: {labels.shape}\")\n",
    "print(f\"Labels tensor dtype: {labels.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create DataLoader for Training\n",
    "\n",
    "Create a PyTorch DataLoader that will be used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Batch size: {dataloader.batch_size}\")\n",
    "\n",
    "# Show a sample batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"Input IDs: {sample_batch[0].shape}\")\n",
    "print(f\"Attention masks: {sample_batch[1].shape}\")\n",
    "print(f\"Labels: {sample_batch[2].shape}\")\n",
    "\n",
    "# Show label distribution in sample batch\n",
    "print(f\"\\nSample batch label distribution:\")\n",
    "unique_labels, counts = torch.unique(sample_batch[2], return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label.item()}: {count.item()} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "At this point, we have successfully:\n",
    "\n",
    "1. âœ… Loaded and explored the tweet emotion intensity dataset\n",
    "2. âœ… Cleaned and preprocessed the text data\n",
    "3. âœ… Tokenized the text using BERT tokenizer\n",
    "4. âœ… Split the data into train/validation/test sets\n",
    "5. âœ… Applied data augmentation using synonym replacement\n",
    "6. âœ… Converted labels to numeric format\n",
    "7. âœ… Created PyTorch DataLoader for training\n",
    "\n",
    "The data is now ready for fine-tuning a BERT model for emotion intensity classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=== DATA PREPARATION SUMMARY ===\")\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTokenization parameters:\")\n",
    "print(f\"- Max sequence length: 128\")\n",
    "print(f\"- Tokenizer: bert-base-uncased\")\n",
    "print(f\"- Padding: True\")\n",
    "print(f\"- Truncation: True\")\n",
    "print(f\"\\nDataLoader parameters:\")\n",
    "print(f\"- Batch size: 16\")\n",
    "print(f\"- Shuffle: True\")\n",
    "print(f\"- Number of batches: {len(dataloader)}\")\n",
    "print(\"\\nðŸš€ Ready for model fine-tuning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
