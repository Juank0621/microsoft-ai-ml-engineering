{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case Demonstration: Selecting and Preparing Data for Fine-tuning\n",
    "\n",
    "This notebook demonstrates the process of selecting and preparing data for fine-tuning a machine learning model, specifically for emotion intensity classification in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Modules\n",
    "\n",
    "First, let's install the necessary packages. The `!` prefix runs the command in the system shell rather than the Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install modules\n",
    "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
    "!pip install nltk transformers torch scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all the necessary libraries for data processing, model tokenization, and machine learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preview Dataset\n",
    "\n",
    "Load the tweet emotion intensity dataset. You can download this dataset from: https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
    "data = pd.read_csv('data/tweet_emotion_intensity.csv')\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())\n",
    "print(f\"\\nDataset shape: {data.shape}\")\n",
    "print(f\"\\nColumn names: {data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "Clean the text data by removing URLs, HTML tags, special characters, and converting to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabet characters\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Show examples of original vs cleaned text\n",
    "print(\"Original vs Cleaned Text Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {data['tweet'].iloc[i]}\")\n",
    "    print(f\"Cleaned:  {data['cleaned_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization with BERT\n",
    "\n",
    "Use the BERT tokenizer to convert text into tokens that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(data['cleaned_text'].tolist(), max_length=128, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Tokenized input shape: {tokens['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {tokens['attention_mask'].shape}\")\n",
    "\n",
    "# Show an example of tokenization\n",
    "example_idx = 0\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"Original text: {data['cleaned_text'].iloc[example_idx]}\")\n",
    "print(f\"Token IDs: {tokens['input_ids'][example_idx][:20]}...\")  # Show first 20 tokens\n",
    "print(f\"Decoded tokens: {tokenizer.decode(tokens['input_ids'][example_idx][:20])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Splitting\n",
    "\n",
    "Split the dataset into training (70%), validation (15%), and test (15%) sets using stratified sampling to maintain class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (70%), validation (15%), and test (15%) sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, stratify=data['sentiment_intensity'], random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['sentiment_intensity'], random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nClass distribution:\")\n",
    "print(\"Training set:\")\n",
    "print(train_data['sentiment_intensity'].value_counts().sort_index())\n",
    "print(\"\\nValidation set:\")\n",
    "print(val_data['sentiment_intensity'].value_counts().sort_index())\n",
    "print(\"\\nTest set:\")\n",
    "print(test_data['sentiment_intensity'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation with Synonym Replacement\n",
    "\n",
    "Augment the training data by replacing some words with their synonyms to increase dataset diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wordnet if this is your first time using it\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def synonym_replacement(word):\n",
    "    \"\"\"Replace a word with its first synonym if available\"\"\"\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        return synonyms[0].lemmas()[0].name()\n",
    "    return word\n",
    "\n",
    "# Apply synonym replacement on random words in the dataset\n",
    "def augment_text(text, replacement_prob=0.2):\n",
    "    \"\"\"Augment text by randomly replacing words with synonyms\"\"\"\n",
    "    words = text.split()\n",
    "    augmented_words = [synonym_replacement(word) if random.random() < replacement_prob else word for word in words]\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply augmentation\n",
    "random.seed(42)  # For reproducibility\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)\n",
    "\n",
    "# Show examples of augmentation\n",
    "print(\"Text Augmentation Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {data['cleaned_text'].iloc[i]}\")\n",
    "    print(f\"Augmented: {data['augmented_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Label Preparation and Dataset Creation\n",
    "\n",
    "Convert sentiment intensity labels to numeric values and prepare the final dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs and labels into PyTorch tensors\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "\n",
    "# Define a mapping function for sentiment intensity\n",
    "def map_sentiment(value):\n",
    "    \"\"\"Map sentiment intensity categories to numeric values\"\"\"\n",
    "    if value == \"high\":\n",
    "        return 1\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Handle unexpected values, if any\n",
    "\n",
    "# Apply the function to each item in 'sentiment_intensity'\n",
    "data['sentiment_intensity_numeric'] = data['sentiment_intensity'].apply(map_sentiment)\n",
    "\n",
    "# Check for any null values after mapping\n",
    "print(f\"Null values after mapping: {data['sentiment_intensity_numeric'].isnull().sum()}\")\n",
    "print(f\"Value distribution after mapping:\")\n",
    "print(data['sentiment_intensity_numeric'].value_counts().sort_index())\n",
    "\n",
    "# Drop any rows where 'sentiment_intensity_numeric' is None\n",
    "data = data.dropna(subset=['sentiment_intensity_numeric']).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'sentiment_intensity_numeric' column to a tensor\n",
    "labels = torch.tensor(data['sentiment_intensity_numeric'].tolist(), dtype=torch.float32)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {data.shape}\")\n",
    "print(f\"Labels tensor shape: {labels.shape}\")\n",
    "print(f\"Labels tensor dtype: {labels.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create DataLoader for Training\n",
    "\n",
    "Create a PyTorch DataLoader that will be used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Batch size: {dataloader.batch_size}\")\n",
    "\n",
    "# Show a sample batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"Input IDs: {sample_batch[0].shape}\")\n",
    "print(f\"Attention masks: {sample_batch[1].shape}\")\n",
    "print(f\"Labels: {sample_batch[2].shape}\")\n",
    "\n",
    "# Show label distribution in sample batch\n",
    "print(f\"\\nSample batch label distribution:\")\n",
    "unique_labels, counts = torch.unique(sample_batch[2], return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label.item()}: {count.item()} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "At this point, we have successfully:\n",
    "\n",
    "1. ✅ Loaded and explored the tweet emotion intensity dataset\n",
    "2. ✅ Cleaned and preprocessed the text data\n",
    "3. ✅ Tokenized the text using BERT tokenizer\n",
    "4. ✅ Split the data into train/validation/test sets\n",
    "5. ✅ Applied data augmentation using synonym replacement\n",
    "6. ✅ Converted labels to numeric format\n",
    "7. ✅ Created PyTorch DataLoader for training\n",
    "\n",
    "The data is now ready for fine-tuning a BERT model for emotion intensity classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=== DATA PREPARATION SUMMARY ===\")\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"\\nTokenization parameters:\")\n",
    "print(f\"- Max sequence length: 128\")\n",
    "print(f\"- Tokenizer: bert-base-uncased\")\n",
    "print(f\"- Padding: True\")\n",
    "print(f\"- Truncation: True\")\n",
    "print(f\"\\nDataLoader parameters:\")\n",
    "print(f\"- Batch size: 16\")\n",
    "print(f\"- Shuffle: True\")\n",
    "print(f\"- Number of batches: {len(dataloader)}\")\n",
    "print(\"\\n🚀 Ready for model fine-tuning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
