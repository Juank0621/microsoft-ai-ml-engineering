{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: LLM fine-tuning (Optional)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever wondered how an AI assistant determines the tone of a review or identifies whether feedback is positive or negative? It all starts with preprocessingâ€”the cornerstone of any successful machine learning model.\n",
    "\n",
    "In machine learning, preprocessing data is an essential step that directly influences your model's performance. Ensuring that your data is clean, structured, and ready for training will set the stage for more accurate predictions and better outcomes. Imagine you are preparing a dataset of patient feedback for sentiment analysis in the healthcare industry. These preprocessing steps will ensure your data is clean and ready for fine-tuning a large language model (LLM) to perform sentiment classification tasks.\n",
    "\n",
    "This guide will take you through the essential steps in preparing datasets for machine learning tasks, focusing on text data for natural language processing (NLP) applications.\n",
    "\n",
    "By the end of this walkthrough, you will be able to:\n",
    "\n",
    "- Clean and preprocess text data for machine learning tasks.\n",
    "- Apply tokenization, text normalization, and missing data handling techniques to ensure your data is ready for model training.\n",
    "- Organize and split your dataset into appropriate training, validation, and test sets for optimal model performance.\n",
    "- Create a structured workflow that prepares datasets for a variety of machine learning tasks, including fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key steps in preprocessing data for machine learning\n",
    "\n",
    "This walkthrough will guide you through the following steps:\n",
    "\n",
    "- Step 1: Clean text\n",
    "- Step 2: Apply tokenization\n",
    "- Step 3: Handle missing data\n",
    "- Step 4: Normalize text\n",
    "- Step 5: Prepare the data for fine-tuning\n",
    "- Step 6: Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clean text\n",
    "\n",
    "Text cleaning is one of the first and most important steps in preparing a dataset for machine learning. Cleaning ensures that your text data is consistent and free from unnecessary noise. This process typically involves removing special characters, URLs, and extra spaces and converting the text to lowercase to ensure uniformity. Note that the following code requires at least a passing familiarity with regular expressions. You can [read more about regular expressions in the Python documentation](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "### Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Create a noisy sample dataset\n",
    "data_dict = {\n",
    "    \"text\": [\n",
    "        \" The staff was very kind and attentive to my needs!!! \",\n",
    "        \"The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com\",\n",
    "        \"The doctor answered all my questions...but the facility was outdated. \",\n",
    "        \"The nurse was compassionate & made me feel comfortable!! :) \",\n",
    "        \"I had to wait over an hour before being seen. Unacceptable service! #frustrated\",\n",
    "        \"The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com\",\n",
    "        \"Everyone I interacted with was professional and helpful. ðŸ˜Š \"\n",
    "    ],\n",
    "    \"label\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "print(data[['cleaned_text', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this important?\n",
    "\n",
    "Cleaning the text ensures that the data provided to the machine learning model is consistent, removing unwanted characters or formatting that could confuse the model. Clean data leads to better feature extraction and, ultimately, improved performance during the training and testing phases. This step is particularly important when fine-tuning LLMs, as clean data ensures the model can focus on learning task-specific patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into individual words or tokens that a model can understand. Tokenization helps break down the text into manageable parts for analysis and learning, especially when working with transformer-based models, such as BERT.\n",
    "\n",
    "### Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "data['tokenized'] = data['cleaned_text'].apply(tokenize_function)\n",
    "print(data[['tokenized', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this important?\n",
    "\n",
    "Tokenization transforms raw text into a format that the machine learning model can process. Without tokenization, a model would struggle to interpret the text's meaning, particularly for NLP tasks in which understanding individual words and their contexts is critical. Using a task-specific tokenizer ensures compatibility with LLMs like BERT, which will be fine-tuned later in your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Handle missing data\n",
    "\n",
    "Handling missing data is crucial to maintaining your dataset's integrity. Missing data can distort your results or cause models to misinterpret the information, leading to poor predictions. There are two primary ways to handle missing data: removing the incomplete data and filling the missing values with an appropriate placeholder or statistic (e.g., mean, median, or mode). Some of these measures of central tendency, primarily mode, for example, which counts the most common appearance of a value, are suitable for nominal data analysis.\n",
    "\n",
    "### Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Option 1: Drop rows with missing data\n",
    "data = data.dropna()\n",
    "\n",
    "# Option 2: Fill missing values with a placeholder\n",
    "data['cleaned_text'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this important?\n",
    "\n",
    "Missing data can lead to bias in the model or cause errors during training. By addressing missing data properly, you ensure that your model learns from complete and accurate information, improving its ability to make correct predictions. This is especially crucial when preparing task-specific datasets for fine-tuning, where data quality directly impacts performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Normalize text\n",
    "\n",
    "Text normalization refers to standardizing the format of your text to ensure consistency. This may include converting all text to lowercase, removing contractions (e.g., changing \"don't\" to \"do not\"), and correcting spelling errors. Normalization is especially important for machine learning tasks in which consistency in input data leads to better feature extraction and model performance.\n",
    "\n",
    "### Additional considerations for normalization\n",
    "\n",
    "- **Stemming or lemmatization:** these techniques reduce words to their base forms, which helps the model focus on the core meaning of the words rather than their specific inflected forms.\n",
    "- **Removing stop words:** normalization often removes such words as \"the,\" \"is,\" or \"and,\" as they do not provide significant meaning in most machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare the data for fine-tuning\n",
    "\n",
    "After cleaning and tokenizing the text, you must prepare the data for training. In tasks like fine-tuning, structuring the data correctly ensures compatibility with LLMs like BERT. This involves organizing the tokenized data and labels into a format that the machine learning model can use during training, for example, as PyTorch DataLoader objects.\n",
    "\n",
    "### Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Prepare tensors for fine-tuning\n",
    "input_ids = torch.cat([token['input_ids'] for token in data['tokenized']], dim=0)\n",
    "attention_masks = torch.cat([token['attention_mask'] for token in data['tokenized']], dim=0)\n",
    "labels = torch.tensor([0 if label == \"negative\" else 1 if label == \"neutral\" else 2 for label in data['label']])\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"DataLoader created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this important?\n",
    "\n",
    "Structuring the data in this way ensures that the model can efficiently process it during training, allowing for smoother fine-tuning and faster convergence. By preparing your data in this format, you enable the model to handle large datasets effectively, even in real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split the data\n",
    "\n",
    "Splitting your dataset into training, validation, and test sets is critical for ensuring your model generalizes well to unseen data. Proper data splitting allows you to monitor the model's performance during training and prevents overfitting, which occurs when the model learns patterns in the training data too well but fails to generalize.\n",
    "\n",
    "### Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    input_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"Data splitting successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this important?\n",
    "\n",
    "By splitting the data, you ensure that the model's performance can be evaluated on unseen data, reducing the risk of overfitting and increasing the likelihood that the model generalizes well to new data. For fine-tuning, validation data is essential for hyperparameter tuning, while test data evaluates the final model's performance. This practice is essential for building robust machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Preprocessing data is a critical step in ensuring that your machine learning model receives high-quality input. From cleaning and tokenizing text to handling missing data and structuring it for training, each preprocessing step plays a vital role in the overall performance of your model. Properly preparing data ensures smoother fine-tuning and better model outcomes, whether you're working on NLP tasks, classification, or regression models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
