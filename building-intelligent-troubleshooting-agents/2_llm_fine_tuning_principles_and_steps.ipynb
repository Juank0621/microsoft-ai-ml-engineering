{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning: Principles and Steps\n",
    "\n",
    "**Disclaimer:** This reading involves resource-intensive tasks like model training. If you are using limited hardware, some tasks may take longer to complete.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Have you ever wondered how an AI assistant determines the tone of a review or identifies whether feedback is positive or negative? It all starts with preprocessingâ€”the cornerstone of any successful machine learning model.\n",
    "\n",
    "In machine learning, preprocessing data is an essential step that directly influences your model's performance. Ensuring that your data is clean, structured, and ready for training sets the stage for accurate predictions and better outcomes. Imagine you are preparing a dataset of patient feedback for sentiment analysis in the healthcare industry. These preprocessing steps will ensure your data is clean and ready for fine-tuning a large language model (LLM) to perform sentiment classification tasks.\n",
    "\n",
    "This reading will take you through the essential steps in preparing datasets for machine learning tasks, focusing on text data for natural language processing (NLP) applications.\n",
    "\n",
    "By the end of this reading, you will be able to:\n",
    "\n",
    "- Clean and preprocess text data for machine learning tasks.\n",
    "- Apply tokenization, text normalization, and missing data handling techniques to ensure your data is ready for model training.\n",
    "- Organize and split your dataset into appropriate training, validation, and test sets for optimal model performance.\n",
    "- Fine-tune and evaluate a large language model for specific tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step process to fine-tuning\n",
    "\n",
    "This reading will guide you through the following steps:\n",
    "\n",
    "- Step 1: Prepare and clean the dataset\n",
    "- Step 2: Tokenize the data\n",
    "- Step 3: Fine-tune the model\n",
    "- Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare and clean the dataset\n",
    "\n",
    "Noisy datasets can degrade model performance. Cleaning ensures consistent input for better predictions. In this step, you'll:\n",
    "\n",
    "- Remove URLs, hashtags, and special characters.\n",
    "- Normalize text by converting it to lowercase.\n",
    "\n",
    "### Code example: Data cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# Example dataset (replace with your own if you want)\n",
    "data_dict = {\n",
    "    \"text\": [\n",
    "        \" The staff was very kind and attentive to my needs!!! \",\n",
    "        \"The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com\",\n",
    "        \"The doctor answered all my questions...but the facility was outdated. \",\n",
    "        \"The nurse was compassionate & made me feel comfortable!! :) \",\n",
    "        \"I had to wait over an hour before being seen. Unacceptable service! #frustrated\",\n",
    "        \"The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com\",\n",
    "        \"Everyone I interacted with was professional and helpful. \"\n",
    "    ],\n",
    "    \"label\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Clean the text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove extra spaces\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "data[\"cleaned_text\"] = data[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize the data\n",
    "\n",
    "Tokenization converts text into a format models can process. This step uses Hugging Face's tokenizer to transform text into token IDs. Before running this code you may need to install Transformers. Make sure after you do this to refresh your page and then continue to run code.\n",
    "\n",
    "!pip install transformers datasets scikit-learn torch accelerate\n",
    "\n",
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Apply tokenization with padding\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "data[\"tokenized\"] = data[\"cleaned_text\"].apply(tokenize_function)\n",
    "\n",
    "# Extract tokenized features\n",
    "data[\"input_ids\"] = data[\"tokenized\"].apply(lambda x: x[\"input_ids\"])\n",
    "data[\"attention_mask\"] = data[\"tokenized\"].apply(lambda x: x[\"attention_mask\"])\n",
    "\n",
    "# Drop old tokenized column\n",
    "data = data.drop(columns=[\"tokenized\"])\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fine-tune the model\n",
    "\n",
    "Using the tokenized data, you'll fine-tune a pretrained BERT model.\n",
    "\n",
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "test_dataset = test_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "\n",
    "#print(train_dataset)\n",
    "\n",
    "# Enable dynamic padding for batches\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Load pre-trained BERT model (3-class classification)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=3\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the model\n",
    "\n",
    "Evaluate the fine-tuned model's accuracy and F1 score on the test set.\n",
    "\n",
    "### Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = test_dataset['label']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Fine-tuning a large language model (LLM) begins with the critical step of preparing your dataset. From cleaning noisy text to tokenizing and splitting your data, each step is vital for ensuring the model's performance is optimized for specific tasks. This reading has provided you with the tools and knowledge to:\n",
    "\n",
    "- Preprocess text data, ensuring consistency and quality.\n",
    "- Tokenize and structure your dataset for machine learning models.\n",
    "- Fine-tune a pretrained model with relevant hyperparameters.\n",
    "- Evaluate and deploy the model effectively.\n",
    "\n",
    "With these skills, you are well-equipped to prepare datasets for a variety of natural language processing (NLP) tasks. By following a structured workflow, you can confidently adapt large language models to meet specialized objectives, unlocking the full potential of AI in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
