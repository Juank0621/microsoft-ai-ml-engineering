{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Walkthrough: Auditing ML Code for Security Vulnerabilities (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "You were asked to review a block of intentionally flawed ML code and audit it for potential security vulnerabilities. Doing this involved auditing the code for potential security vulnerabilities that could pose risks to data integrity, confidentiality, and the overall security of the ML system.\n",
        "\n",
        "In this reading, we will walk through the lab assignment step by step, discussing the vulnerabilities present in the code, their potential impacts, and the proper solutions to secure the ML system.\n",
        "\n",
        "By the end of this walkthrough, you will be able to:\n",
        "\n",
        "- Identify common security risks in ML code and mitigate the risks effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review the provided ML code block\n",
        "\n",
        "**Original code block:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "\n",
        "# Load dataset (Flaw: No data validation or sanitization)\n",
        "data = pd.read_csv('user_data.csv')\n",
        "\n",
        "# Split the dataset into features and target (Flaw: No input validation)\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets (Flaw: Fixed random state)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple logistic regression model (Flaw: No model security checks)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to disk (Flaw: Unencrypted model saving)\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "# Load the model from disk for later use (Flaw: No integrity checks on the loaded model)\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {result:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** This code block contains several security vulnerabilities that could compromise the integrity and confidentiality of the ML system. We'll go through each identified flaw and discuss how to address them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify security vulnerabilities\n",
        "\n",
        "### Vulnerability 1: Lack of data validation and sanitization\n",
        "\n",
        "- **Issue:** The dataset is loaded directly from a file without any validation or sanitization. This opens the door to potential injection attacks or the use of corrupted data.\n",
        "- **Impact:** Malicious data could compromise the model's accuracy or introduce security risks by executing unwanted code.\n",
        "\n",
        "**Solution:** Implement data validation and sanitization routines to ensure that the input data is clean and free of malicious content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate and sanitize input data\n",
        "def validate_data(df):\n",
        "    if df.isnull().values.any():\n",
        "        raise ValueError(\"Dataset contains null values. Please clean the data before processing.\")\n",
        "    \n",
        "    # Additional validation checks can be added here\n",
        "    return df\n",
        "\n",
        "# Load and validate dataset\n",
        "data = validate_data(pd.read_csv('user_data.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vulnerability 2: No input validation\n",
        "\n",
        "- **Issue:** The code assumes that the input data will always be in the correct format. However, if the data is corrupted or contains unexpected types, this could lead to errors or vulnerabilities.\n",
        "- **Impact:** Invalid input data could cause the model training process to fail or produce inaccurate results.\n",
        "\n",
        "**Solution:** Include input validation checks to ensure that the data meets expected formats and values before processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset into features and target with validation\n",
        "X = validate_data(data.iloc[:, :-1])\n",
        "y = validate_data(data.iloc[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vulnerability 3: Fixed random state\n",
        "\n",
        "- **Issue:** The use of a fixed random state (e.g., random_state=42) can make the model training process predictable, potentially exposing it to attacks that exploit this predictability.\n",
        "- **Impact:** Attackers can use predictable random states to infer model behaviors or replicate the training process maliciously.\n",
        "\n",
        "**Solution:** Use a securely generated random state, such as one derived from a secure source such as os.urandom()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Split the data into training and testing sets with a securely managed random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=os.urandom(16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vulnerability 4: Unencrypted model saving\n",
        "\n",
        "- **Issue:** The trained model is saved to disk without encryption, which means that anyone with access to the storage location can access and potentially tamper with the model.\n",
        "- **Impact:** Unencrypted models are vulnerable to unauthorized access, theft, and tampering.\n",
        "\n",
        "**Solution:** Encrypt the model before saving it to disk to protect its confidentiality and integrity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cryptography.fernet\n",
        "\n",
        "# Encrypt model before saving\n",
        "key = cryptography.fernet.Fernet.generate_key()\n",
        "cipher = cryptography.fernet.Fernet(key)\n",
        "\n",
        "# Save the encrypted model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "encrypted_model = cipher.encrypt(pickle.dumps(model))\n",
        "with open(filename, 'wb') as f:\n",
        "    f.write(encrypted_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vulnerability 5: No integrity checks on loaded model\n",
        "\n",
        "- **Issue:** The model is loaded from disk without any integrity checks, meaning that it could be tampered with or corrupted without detection.\n",
        "- **Impact:** A compromised model could produce incorrect predictions or expose vulnerabilities in downstream applications.\n",
        "\n",
        "**Solution:** Implement integrity checks to verify that the model has not been altered between saving and loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "# Load the encrypted model from disk and verify its integrity\n",
        "with open(filename, 'rb') as f:\n",
        "    encrypted_model = f.read()\n",
        "\n",
        "decrypted_model = cipher.decrypt(encrypted_model)\n",
        "loaded_model = pickle.loads(decrypted_model)\n",
        "\n",
        "# Compute hash of the loaded model\n",
        "loaded_model_hash = hashlib.sha256(decrypted_model).hexdigest()\n",
        "\n",
        "# Verify that the loaded model's hash matches the original\n",
        "original_model_hash = hashlib.sha256(pickle.dumps(model)).hexdigest()\n",
        "\n",
        "if loaded_model_hash != original_model_hash:\n",
        "    raise ValueError(\"Model integrity check failed. The model may have been tampered with.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Proper solution overview\n",
        "\n",
        "By addressing these vulnerabilities, the improved ML code ensures that the data is validated and sanitized, the model is trained and stored securely, and integrity checks are performed during model loading. Here's the finalized code with all the security improvements applied:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import hashlib\n",
        "import cryptography.fernet\n",
        "\n",
        "# Validate and sanitize input data\n",
        "def validate_data(df):\n",
        "    if df.isnull().values.any():\n",
        "        raise ValueError(\"Dataset contains null values. Please clean the data before processing.\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load and validate dataset\n",
        "data = validate_data(pd.read_csv('user_data.csv'))\n",
        "\n",
        "# Split the dataset into features and target with validation\n",
        "X = validate_data(data.iloc[:, :-1])\n",
        "y = validate_data(data.iloc[:, -1])\n",
        "\n",
        "# Split the data into training and testing sets with a securely managed random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=os.urandom(16))\n",
        "\n",
        "# Train a logistic regression model with added security considerations\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Encrypt model before saving\n",
        "key = cryptography.fernet.Fernet.generate_key()\n",
        "cipher = cryptography.fernet.Fernet(key)\n",
        "\n",
        "# Save the encrypted model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "encrypted_model = cipher.encrypt(pickle.dumps(model))\n",
        "with open(filename, 'wb') as f:\n",
        "    f.write(encrypted_model)\n",
        "\n",
        "# Load the encrypted model from disk and verify its integrity\n",
        "with open(filename, 'rb') as f:\n",
        "    encrypted_model = f.read()\n",
        "\n",
        "decrypted_model = cipher.decrypt(encrypted_model)\n",
        "loaded_model = pickle.loads(decrypted_model)\n",
        "\n",
        "# Compute hash of the loaded model\n",
        "loaded_model_hash = hashlib.sha256(decrypted_model).hexdigest()\n",
        "\n",
        "# Verify that the loaded model's hash matches the original\n",
        "original_model_hash = hashlib.sha256(pickle.dumps(model)).hexdigest()\n",
        "\n",
        "if loaded_model_hash != original_model_hash:\n",
        "    raise ValueError(\"Model integrity check failed. The model may have been tampered with.\")\n",
        "\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {result:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This activity highlights the importance of implementing security best practices in every stage of the ML life cycle, from data collection and preprocessing to model training and deployment. By following the steps outlined in this walkthrough, you should now have a better understanding of how to identify and mitigate security vulnerabilities in ML code.\n",
        "\n",
        "Securing ML systems is not just about preventing unauthorized access; it's also about ensuring the integrity and reliability of the models you build. By incorporating these practices into your development workflow, you can protect your models from a wide range of potential threats and ensure that they remain trustworthy and effective in real-world applications."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
