{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practice Activity: Setup a Local Data Cleaning and Preprocessing Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Data cleaning and preprocessing are essential steps in preparing your data for analysis and machine learning. Setting up a local environment for these tasks allows you to automate and streamline your workflow. In this reading, we'll guide you through the process of setting up a local data cleaning and preprocessing tool using Python. This setup will involve installing necessary libraries, creating a reusable script, and automating common data preprocessing tasks.\n",
        "\n",
        "By the end of this hands-on activity, you will be able to:\n",
        "\n",
        "- Set up a local environment for data cleaning and preprocessing.\n",
        "- Load and clean datasets by handling missing values and outliers.\n",
        "- Normalize and encode data for machine learning applications.\n",
        "- Automate data preprocessing tasks using reusable Python functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1. Prerequisites\n",
        "\n",
        "Before you begin, make sure you have the following installed on your local machine:\n",
        "\n",
        "- **Python 3.x**: the programming language we'll use for scripting\n",
        "- **pip**: Python's package installer, which you'll use to install the necessary libraries\n",
        "- **A code editor**: such as VS Code, PyCharm, or a text editor such as Sublime Text\n",
        "\n",
        "**Key Python libraries for data cleaning and preprocessing:**\n",
        "\n",
        "- **pandas**: for data manipulation and analysis\n",
        "- **NumPy**: for numerical operations\n",
        "- **Scikit-learn**: for data preprocessing and machine learning tasks\n",
        "- **Missingno** (optional): for visualizing missing data\n",
        "\n",
        "Install these libraries using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "# Run this command in your terminal or command prompt:\n",
        "# pip install pandas numpy scikit-learn missingno\n",
        "\n",
        "print(\"Please install the required libraries using:\")\n",
        "print(\"pip install pandas numpy scikit-learn missingno\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2. Create the data cleaning and preprocessing script\n",
        "\n",
        "Once you have the necessary libraries installed, you can start creating your data cleaning and preprocessing tool.\n",
        "\n",
        "## Step-by-step guide\n",
        "\n",
        "### Step 1: Import the required libraries\n",
        "\n",
        "Begin by importing the libraries you'll use in your script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import missingno as msno  # Optional: for visualizing missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** These libraries provide the functions and methods you'll need to clean and preprocess your data, such as handling missing values, scaling data, and visualizing missing data.\n",
        "\n",
        "### Step 2: Load the dataset\n",
        "\n",
        "Load the dataset you want to clean and preprocess:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your dataset into a pandas DataFrame\n",
        "df = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The pd.read_csv() function loads the dataset into a pandas DataFrame, which is a powerful data structure for manipulating and analyzing data in Python.\n",
        "\n",
        "### Step 3: Handle missing values\n",
        "\n",
        "One of the first steps in data cleaning is handling missing values. You can choose to remove, fill, or visualize missing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize missing data (optional)\n",
        "msno.matrix(df)\n",
        "msno.heatmap(df)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# Or, fill missing values with the mean\n",
        "df_filled = df.fillna(df.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The msno library provides visualization tools to understand where missing data is in your dataset. The dropna() and fillna() methods allow you to handle missing values by either removing them or filling them with a substitute value.\n",
        "\n",
        "### Step 4: Handle outliers\n",
        "\n",
        "Outliers can be managed by either removing them or transforming them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify outliers using Z-score\n",
        "from scipy import stats\n",
        "\n",
        "z_scores = np.abs(stats.zscore(df_cleaned))\n",
        "df_no_outliers = df_cleaned[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "# Or cap outliers at a threshold\n",
        "upper_limit = df_cleaned['column_name'].quantile(0.95)\n",
        "df_cleaned['column_name'] = np.where(df_cleaned['column_name'] > upper_limit, upper_limit, df_cleaned['column_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The Z-score method helps identify outliers by calculating how many standard deviations a data point is from the mean. You can remove these outliers or cap them to reduce their impact on your analysis.\n",
        "\n",
        "### Step 5: Scale and normalize data\n",
        "\n",
        "Normalize or scale your data to ensure that all features contribute equally to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df_cleaned), columns=df_cleaned.columns)\n",
        "\n",
        "# Z-score Standardization\n",
        "scaler = StandardScaler()\n",
        "df_standardized = pd.DataFrame(scaler.fit_transform(df_cleaned), columns=df_cleaned.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** Scaling ensures that all numerical features in your dataset are on the same scale, which is important for many machine learning algorithms. Min-Max Scaling scales data to a [0, 1] range, while Z-score Standardization scales data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "### Step 6: Encode categorical variables\n",
        "\n",
        "Convert categorical variables into a numerical format that machine learning algorithms can process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding for categorical variables\n",
        "df_encoded = pd.get_dummies(df_scaled, columns=['categorical_column_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** One-hot encoding converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.\n",
        "\n",
        "### Step 7: Save the cleaned and preprocessed data\n",
        "\n",
        "Once you've cleaned and preprocessed your data, save it to a new file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the cleaned and preprocessed DataFrame to a new CSV file\n",
        "df_encoded.to_csv('cleaned_preprocessed_data.csv', index=False)\n",
        "\n",
        "print('Data cleaning and preprocessing complete. File saved as cleaned_preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The cleaned and preprocessed data is saved to a new CSV file, making it ready for use in analysis or model training.\n",
        "\n",
        "## Part 3. Automate the workflow\n",
        "\n",
        "To streamline your data preprocessing workflow, consider wrapping these steps into functions or a reusable script. Here's a basic structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(filepath):\n",
        "    return pd.read_csv(filepath)\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    return df.fillna(df.mean())\n",
        "\n",
        "def remove_outliers(df):\n",
        "    z_scores = np.abs(stats.zscore(df))\n",
        "    return df[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "def scale_data(df):\n",
        "    scaler = StandardScaler()\n",
        "    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "def encode_categorical(df, categorical_columns):\n",
        "    return pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "def save_data(df, output_filepath):\n",
        "    df.to_csv(output_filepath, index=False)\n",
        "\n",
        "# Example usage:\n",
        "df = load_data('your_dataset.csv')\n",
        "df = handle_missing_values(df)\n",
        "df = remove_outliers(df)\n",
        "df = scale_data(df)\n",
        "df = encode_categorical(df, ['categorical_column_name'])\n",
        "save_data(df, 'cleaned_preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** These functions encapsulate each step of the data cleaning and preprocessing workflow, making it easier to apply the same process to different datasets.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "By setting up a local data cleaning and preprocessing tool, you can automate much of the work involved in preparing your data for analysis. This setup ensures that your data is clean, consistent, and properly formatted, which is essential for building accurate and reliable machine learning models.\n",
        "\n",
        "As you refine your script and add more functionality, you'll become more efficient in handling a wide variety of data challenges, paving the way for successful AI/ML projects."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
