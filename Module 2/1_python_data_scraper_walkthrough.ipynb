{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Walkthrough: Setup of a local python data scraper (Optional)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You've just set up a basic web scraper using Python. Now let's review the \"proper\" solution, which includes a more detailed explanation of the steps involved and refined code snippets. This walkthrough will help ensure that your web scraper is robust, efficient, and capable of handling common challenges that arise during web scraping.\n",
        "\n",
        "By the end of this reading, you will be able to:\n",
        "\n",
        "- Implement a robust and efficient web scraping solution using Python.\n",
        "- Handle common challenges such as missing data, dynamic content, and network errors.\n",
        "- Apply best practices for data acquisition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-step breakdown of the solution\n",
        "\n",
        "### Step 1: Import the necessary libraries\n",
        "\n",
        "Start by importing the essential Python libraries required for web scraping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time # Optional: To add delays between requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **requests:** handles sending HTTP requests to the website\n",
        "- **BeautifulSoup:** parses the HTML content and helps navigate the structure\n",
        "- **pandas:** organizes the scraped data into a DataFrame and allows for easy export to a CSV file\n",
        "- **time:** (optional) adds delays between requests to avoid overloading the website's server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Send an HTTP request\n",
        "\n",
        "Send a GET request to the web page you want to scrape. This request retrieves the HTML content of the page:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://example.com' # Replace with the URL of the target website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verify the request was successful\n",
        "if response.status_code == 200:\n",
        "    print('Successfully retrieved the webpage.')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage. Status code:', response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** the requests.get() function sends a request to the specified URL and stores the response. It's important to check the status_code of the response to ensure that the request was successful (a status code of 200 indicates success)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Parse the HTML content\n",
        "\n",
        "Once you retrieve the HTML content, use BeautifulSoup to parse it and create a navigable tree structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Print the title of the webpage to confirm successful parsing\n",
        "print('Webpage Title:', soup.title.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** BeautifulSoup parses the HTML content and allows you to navigate and search through the HTML elements easily. The soup.title.text line prints the title of the web page to confirm that the HTML has been parsed correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Identify and extract the data\n",
        "\n",
        "Determine which HTML elements contain the data you want to extract. For this example, let's assume you're scraping a table with product information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locate the table that contains the product data\n",
        "table = soup.find('table', {'id': 'product-table'}) # Replace with the actual id or class name\n",
        "\n",
        "# Extract the rows of the table\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Loop through each row and extract the relevant data\n",
        "for row in rows[1:]: # Skipping the header row\n",
        "    cols = row.find_all('td')\n",
        "    product_name = cols[0].text.strip()\n",
        "    price = cols[1].text.strip()\n",
        "    rating = cols[2].text.strip()\n",
        "    \n",
        "    data.append([product_name, price, rating])\n",
        "\n",
        "# Convert the list to a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns=['Product Name', 'Price', 'Rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** this code locates the table with the product data and iterates over each row (skipping the header). For each row, it extracts the product name, price, and rating and appends this information to a list. Finally, the list is converted into a pandas DataFrame for easier manipulation and export."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Handle common scraping challenges\n",
        "\n",
        "Web scraping often involves dealing with various challenges, such as missing data, dynamic content, or blocked requests. Here are a few strategies to handle these:\n",
        "\n",
        "**a. Handling missing data**\n",
        "\n",
        "If some rows or columns might be missing data, you can add checks to handle these cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for row in rows[1:]:\n",
        "    cols = row.find_all('td')\n",
        "    \n",
        "    if len(cols) == 3: # Ensure all three columns are present\n",
        "        product_name = cols[0].text.strip() if cols[0] else 'N/A'\n",
        "        price = cols[1].text.strip() if cols[1] else 'N/A'\n",
        "        rating = cols[2].text.strip() if cols[2] else 'N/A'\n",
        "        \n",
        "        data.append([product_name, price, rating])\n",
        "    else:\n",
        "        print('Skipping a row with missing data.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**b. Adding delays between requests**\n",
        "\n",
        "To avoid overwhelming the server or getting blocked, it's good practice to add delays between requests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time.sleep(2) # Adds a 2-second delay before the next request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**c. Handling dynamic content**\n",
        "\n",
        "Some websites load content dynamically using JavaScript, which can't be directly scraped with BeautifulSoup. In such cases, you might need to use Selenium, a web driver that can interact with JavaScript-driven content.\n",
        "\n",
        "**d. Error handling**\n",
        "\n",
        "Incorporate error handling to manage issues such as network errors or changes in the website structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raises an HTTPError for bad responses\n",
        "except requests.exceptions.HTTPError as err:\n",
        "    print('HTTP error occurred:', err)\n",
        "except Exception as err:\n",
        "    print('Other error occurred:', err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Save the scraped data\n",
        "\n",
        "Finally, save the scraped data to a CSV file for further analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_products.csv', index=False)\n",
        "\n",
        "print('Data successfully saved to scraped_products.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** the to_csv() method saves the DataFrame to a CSV file, making it easy to load the data into other tools or share it with others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Setting up a web scraper in Python is a powerful way to acquire data from the web for your AI/ML projects. This solution guide walks you through the proper steps, from sending HTTP requests to parsing HTML and handling common challenges. By following this structured approach, you can create robust scrapers that effectively gather the data you need, even when working with complex or dynamic websites.\n",
        "\n",
        "As you continue to refine your skills, consider exploring more advanced topics such as handling AJAX calls, interacting with APIs, and using Selenium for dynamic content. The more you practice, the better equipped you'll be to tackle a wide range of data acquisition tasks in your future projects."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
