{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practice Activity: Auditing ML Code for Security Vulnerabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this activity, you will be tasked with reviewing a block of intentionally flawed ML code. Your objective is to audit the code for potential security vulnerabilities that could pose risks to data integrity, confidentiality, and the overall security of the ML system. This activity will take you approximately 60 minutes to complete.\n",
        "\n",
        "By the end of this activity, you will be able to:\n",
        "\n",
        "- Identify and mitigate security risks in AI/ML development.\n",
        "- Explain the implications of security vulnerabilities in ML code.\n",
        "- Propose solutions to mitigate the identified risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructions\n",
        "\n",
        "### Step-by-step guide\n",
        "\n",
        "### Step 1: Review the provided ML code block\n",
        "\n",
        "You will be given a block of ML code that contains several intentional security flaws. Your first task is to carefully review the code and identify any potential vulnerabilities. These may include issues related to data handling, access controls, model deployment, and other security aspects.\n",
        "\n",
        "**Code block example (with intentional flaws)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "\n",
        "# Load dataset (Flaw: No data validation or sanitization)\n",
        "data = pd.read_csv('user_data.csv')\n",
        "\n",
        "# Split the dataset into features and target (Flaw: No input validation)\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets (Flaw: Fixed random state)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple logistic regression model (Flaw: No model security checks)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to disk (Flaw: Unencrypted model saving)\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "# Load the model from disk for later use (Flaw: No integrity checks on the loaded model)\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {result:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Identify security vulnerabilities\n",
        "\n",
        "As you review the code, look for potential security vulnerabilities such as:\n",
        "\n",
        "- **Data validation and sanitization:** Is the input data validated or sanitized to prevent malicious input?\n",
        "- **Input validation:** Are there any checks on the input data to ensure it meets expected formats or values?\n",
        "- **Random state and seed management:** Is the random state used securely to prevent predictability in model training?\n",
        "- **Model security:** Are there security measures in place to protect the model from tampering or unauthorized access?\n",
        "- **Encryption:** Is sensitive data, such as the trained model, encrypted when stored or transmitted?\n",
        "- **Integrity checks:** Are there mechanisms to verify the integrity of the model when loading it from storage?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Document your findings\n",
        "\n",
        "For each vulnerability you identify, document the following:\n",
        "\n",
        "- **Vulnerability description:** Clearly describe the issue you've identified.\n",
        "- **Potential impact:** Explain the potential security risks if this vulnerability were exploited.\n",
        "- **Mitigation strategy:** Propose a solution or best practice to mitigate the identified vulnerability.\n",
        "\n",
        "**Example documentation:**\n",
        "\n",
        "- **Vulnerability:** Lack of data validation and sanitization when loading the dataset.\n",
        "  - **Impact:** Malicious users could inject harmful code or corrupted data, leading to data breaches or compromised model integrity.\n",
        "  - **Mitigation:** Implement input validation and sanitization routines before processing the data. For example, ensure that the CSV file only contains expected data types and values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Propose code improvements\n",
        "\n",
        "Based on your findings, propose improvements to the provided code block to enhance its security. Rewrite sections of the code where necessary, incorporating best practices such as:\n",
        "\n",
        "- Adding data validation and sanitization.\n",
        "- Using secure random state management.\n",
        "- Encrypting models before saving them to disk.\n",
        "- Implementing integrity checks when loading models.\n",
        "\n",
        "**Example code improvement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "import hashlib\n",
        "\n",
        "# Validate and sanitize input data\n",
        "def validate_data(df):\n",
        "    # Example validation: Check for null values, correct data types, etc.\n",
        "    if df.isnull().values.any():\n",
        "        raise ValueError(\"Dataset contains null values. Please clean the data before processing.\")\n",
        "    \n",
        "    # Additional validation checks can be added here\n",
        "    return df\n",
        "\n",
        "# Load and validate dataset\n",
        "data = validate_data(pd.read_csv('user_data.csv'))\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets with a securely managed random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=os.urandom(16))\n",
        "\n",
        "# Train a logistic regression model with added security considerations\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to disk with encryption\n",
        "filename = 'finalized_model.sav'\n",
        "with open(filename, 'wb') as model_file:\n",
        "    encrypted_model = pickle.dumps(model)\n",
        "    model_file.write(encrypted_model)\n",
        "\n",
        "# Load the model from disk and verify its integrity\n",
        "with open(filename, 'rb') as model_file:\n",
        "    loaded_model = pickle.loads(model_file.read())\n",
        "\n",
        "if hashlib.sha256(pickle.dumps(loaded_model)).hexdigest() != hashlib.sha256(pickle.dumps(model)).hexdigest():\n",
        "    raise ValueError(\"Model integrity check failed. The model may have been tampered with.\")\n",
        "\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {result:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Compile your findings\n",
        "\n",
        "Compile your findings, including the identified vulnerabilities, their potential impacts, proposed mitigation strategies, and the improved code.\n",
        "\n",
        "## Security Vulnerabilities Analysis\n",
        "\n",
        "Below is a comprehensive analysis of the security vulnerabilities found in the original code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Lack of Data Validation and Sanitization\n",
        "\n",
        "**Vulnerability:** The code directly loads data from a CSV file without any validation or sanitization.\n",
        "\n",
        "**Impact:** \n",
        "- Malicious users could inject harmful code or corrupted data\n",
        "- Could lead to data breaches or compromised model integrity\n",
        "- Potential for code injection attacks through malicious CSV content\n",
        "\n",
        "**Mitigation:** \n",
        "- Implement input validation functions to check data types, ranges, and formats\n",
        "- Sanitize data before processing\n",
        "- Use schema validation to ensure data conforms to expected structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Predictable Random State\n",
        "\n",
        "**Vulnerability:** Using a fixed random state (random_state=42) makes the model training process predictable.\n",
        "\n",
        "**Impact:** \n",
        "- Attackers could predict the train/test split\n",
        "- Reduces the security of the model by making it deterministic\n",
        "- Could be exploited for adversarial attacks\n",
        "\n",
        "**Mitigation:** \n",
        "- Use cryptographically secure random number generation (os.urandom())\n",
        "- Avoid fixed random states in production environments\n",
        "- Implement proper seed management for reproducibility when needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Unencrypted Model Storage\n",
        "\n",
        "**Vulnerability:** The trained model is saved to disk without encryption using pickle.\n",
        "\n",
        "**Impact:** \n",
        "- Model parameters and architecture are exposed in plaintext\n",
        "- Sensitive information could be extracted from the model\n",
        "- Risk of model theft or reverse engineering\n",
        "\n",
        "**Mitigation:** \n",
        "- Encrypt model files before saving to disk\n",
        "- Use secure serialization methods\n",
        "- Implement access controls for model files\n",
        "- Consider using secure model storage solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Lack of Model Integrity Checks\n",
        "\n",
        "**Vulnerability:** No integrity verification when loading the model from disk.\n",
        "\n",
        "**Impact:** \n",
        "- Model could be tampered with or corrupted without detection\n",
        "- Malicious models could be substituted\n",
        "- No way to verify model authenticity\n",
        "\n",
        "**Mitigation:** \n",
        "- Implement hash-based integrity checks\n",
        "- Use digital signatures for model verification\n",
        "- Store checksums separately and verify on load\n",
        "- Implement model versioning and audit trails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Unsafe File Operations\n",
        "\n",
        "**Vulnerability:** Direct file operations without proper error handling or security checks.\n",
        "\n",
        "**Impact:** \n",
        "- Risk of path traversal attacks\n",
        "- Potential for file system manipulation\n",
        "- No handling of file access errors\n",
        "\n",
        "**Mitigation:** \n",
        "- Use secure file handling practices\n",
        "- Implement proper error handling\n",
        "- Validate file paths and names\n",
        "- Use context managers for file operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Security Best Practices\n",
        "\n",
        "Beyond the specific vulnerabilities identified, here are additional security best practices for ML code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of additional security measures that could be implemented\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from cryptography.fernet import Fernet\n",
        "\n",
        "# Set up secure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Example of secure configuration management\n",
        "class SecureConfig:\n",
        "    def __init__(self):\n",
        "        self.allowed_file_extensions = ['.csv', '.json']\n",
        "        self.max_file_size = 100 * 1024 * 1024  # 100MB\n",
        "        self.encryption_key = self._generate_key()\n",
        "    \n",
        "    def _generate_key(self):\n",
        "        \"\"\"Generate or load encryption key securely\"\"\"\n",
        "        key_file = Path('encryption.key')\n",
        "        if key_file.exists():\n",
        "            return key_file.read_bytes()\n",
        "        else:\n",
        "            key = Fernet.generate_key()\n",
        "            key_file.write_bytes(key)\n",
        "            return key\n",
        "\n",
        "# Example of secure file validation\n",
        "def validate_file_path(file_path):\n",
        "    \"\"\"Validate file path to prevent directory traversal attacks\"\"\"\n",
        "    path = Path(file_path).resolve()\n",
        "    \n",
        "    # Check if path is within allowed directory\n",
        "    allowed_dir = Path.cwd().resolve()\n",
        "    if not str(path).startswith(str(allowed_dir)):\n",
        "        raise ValueError(\"File path not allowed\")\n",
        "    \n",
        "    # Check file extension\n",
        "    if path.suffix not in ['.csv', '.json']:\n",
        "        raise ValueError(\"File type not allowed\")\n",
        "    \n",
        "    return path\n",
        "\n",
        "print(\"Security best practices implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This activity is designed to enhance your ability to identify and address security vulnerabilities in ML code. By completing this exercise, you will gain practical experience in securing AI/ML systems, an essential skill in today's increasingly data-driven world. Remember, the goal is not just to write functional code, but to ensure that it is secure and robust against potential threats.\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Always validate and sanitize input data** to prevent malicious code injection\n",
        "2. **Use secure random number generation** to avoid predictable behavior\n",
        "3. **Encrypt sensitive data** including trained models when storing or transmitting\n",
        "4. **Implement integrity checks** to detect tampering or corruption\n",
        "5. **Follow secure coding practices** including proper error handling and logging\n",
        "6. **Regularly audit your code** for security vulnerabilities\n",
        "7. **Stay updated** with the latest security best practices in AI/ML development\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Practice implementing these security measures in your own ML projects\n",
        "- Learn about additional security frameworks and tools for AI/ML\n",
        "- Stay informed about emerging security threats in the AI/ML domain\n",
        "- Consider security implications early in the development process, not as an afterthought"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
