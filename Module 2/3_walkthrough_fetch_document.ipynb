{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Walkthrough: Fetch a Document Using the Python Web Scraper (Optional)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You've just been guided through the process of building a Python web scraper capable of fetching documents from a website. In this reading, we will provide a detailed explanation of the \"proper\" solution to the lab assignment, including the code and the rationale behind each step. This walkthrough will help ensure that your web scraper is robust and functions correctly when fetching documents such as PDFs or text files.\n",
        "\n",
        "By the end of this reading, you will be able to:\n",
        "\n",
        "- Explain the step-by-step breakdown of a Python web scraper.\n",
        "- Implement error handling and manage different file types effectively.\n",
        "- Recognize best practices for ethical web scraping.\n",
        "\n",
        "## Step-by-step breakdown of the solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Import required libraries\n",
        "\n",
        "To start, make sure you import all the necessary Python libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **requests:** Used for sending HTTP requests to the website\n",
        "- **BeautifulSoup:** A library for parsing HTML and navigating the structure of the web page\n",
        "- **os:** Helps with path management and file operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Send an HTTP request to the webpage\n",
        "\n",
        "The first step in fetching a document is to access the webpage where the document is hosted. Use the requests library to send a GET request to the webpage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://example.com/documents' # Replace with the actual URL of the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verify the request was successful\n",
        "if response.status_code == 200:\n",
        "    print('Successfully retrieved the webpage.')\n",
        "else:\n",
        "    print('Failed to retrieve the webpage. Status code:', response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The requests.get() function sends a GET request to the specified URL, and the status code of the response is checked to ensure that the page was successfully retrieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Parse the HTML content\n",
        "\n",
        "Once the webpage is retrieved, the next step is to parse the HTML content using BeautifulSoup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Optional: Print the title of the webpage to confirm successful parsing\n",
        "print('Webpage Title:', soup.title.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** BeautifulSoup is used to parse the HTML content, creating a navigable tree structure that allows you to locate the document link within the page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Locate the document link\n",
        "\n",
        "The document (e.g., PDF or text file) is typically linked within an <a> tag. You need to locate this tag and extract the href attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locate the <a> tag that contains the link to the document\n",
        "document_link = soup.find('a', {'class': 'download-link'})['href'] # Replace with the actual class or identifier\n",
        "\n",
        "# Print the document link to verify\n",
        "print('Document link found:', document_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The find() method searches for the first <a> tag with the specified class (download-link, in this case). The href attribute is then extracted, which contains the path to the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Handle relative URLs\n",
        "\n",
        "If the href attribute contains a relative URL, you need to convert it to a full URL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_url = 'https://example.com' # The base URL of the website\n",
        "full_url = os.path.join(base_url, document_link)\n",
        "\n",
        "print('Full URL:', full_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The os.path.join() function is used to combine the base URL with the relative URL, forming a full URL that can be used to download the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Download and save the document\n",
        "\n",
        "With the full URL in hand, you can now send a GET request to download the document. The downloaded file is then saved to your local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send a GET request to download the document\n",
        "document_response = requests.get(full_url)\n",
        "\n",
        "# Check if the document request was successful\n",
        "if document_response.status_code == 200:\n",
        "    # Save the document to a file\n",
        "    with open('document.pdf', 'wb') as file: # Replace 'document.pdf' with the appropriate filename and extension\n",
        "        file.write(document_response.content)\n",
        "    \n",
        "    print('Document downloaded successfully.')\n",
        "else:\n",
        "    print('Failed to download the document. Status code:', document_response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** The requests.get() function retrieves the document, and the open() function is used to save it as a file on your local machine. The file is opened in binary write mode (wb) to correctly handle non-text files such as PDFs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle multiple documents (optional)\n",
        "\n",
        "If the webpage contains multiple documents, you can modify the scraper to loop through all available document links and download each one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find all <a> tags with the document links\n",
        "document_links = soup.find_all('a', {'class': 'download-link'}) # Replace with the actual class or identifier\n",
        "\n",
        "# Loop through each link and download the corresponding document\n",
        "for i, link in enumerate(document_links):\n",
        "    document_url = os.path.join(base_url, link['href'])\n",
        "    document_response = requests.get(document_url)\n",
        "    \n",
        "    if document_response.status_code == 200:\n",
        "        # Save each document with a unique name\n",
        "        file_name = f'document_{i+1}.pdf' # Adjust the file name as needed\n",
        "        \n",
        "        with open(file_name, 'wb') as file:\n",
        "            file.write(document_response.content)\n",
        "        \n",
        "        print(f'Document {i+1} downloaded successfully as {file_name}.')\n",
        "    else:\n",
        "        print(f'Failed to download document {i+1}. Status code:', document_response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation:** This code snippet locates all document links on the page and iterates through them, downloading each document and saving it with a unique filename."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important considerations\n",
        "\n",
        "**Website permissions:** Always ensure that your scraping activities comply with the website's terms of service and relevant laws.\n",
        "\n",
        "**Error handling:** Include error handling in your script to manage network issues or changes in the website's structure.\n",
        "\n",
        "**File types:** Adapt the file-saving logic to handle different file types (e.g., .txt, .csv, and .docx) appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This detailed solution guide provides a complete and correct implementation for fetching documents using the Python web scraper you built in the previous lessons. By following these steps, you can confidently download documents from the web and integrate this functionality into your data acquisition pipeline.\n",
        "\n",
        "As you continue to work on web scraping projects, remember to apply best practices, such as respecting website permissions and handling errors gracefully, to ensure your scrapers are both effective and responsible."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
